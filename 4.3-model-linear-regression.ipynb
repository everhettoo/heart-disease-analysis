{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Algorithm: Linear Regression\n",
    "Acknowledgement: Most of the code was referenced from Prof. Mafas Raheem's AML lab-6.2.1.\n",
    "## Model Validation Strategy\n",
    "Three models evaluated with two different datasets as described below (altogether 6 models).\n",
    "### Standard dataset (without class balancing)\n",
    "- Base\n",
    "- Grid-Search-CV\n",
    "- Random-Search-CV\n",
    "### Oversampled dataset (with class balancing)\n",
    "- Base\n",
    "- Grid-Search-CV\n",
    "- Random-Search-CV\n",
    "### Conclusion (for all evaluations)\n",
    "- accuracy chart"
   ],
   "id": "a5c7eeb486978711"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from numpy import arange\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import custom_libs.evaluation_helper as ev\n",
    "import models.uci_heart_disease_dataset as uci\n",
    "\n",
    "# Warning was disabled to capture errors and warning. Can be enabled when needed.\n",
    "# warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the preprocessed dataset (299 records; with 14 features)\n",
    "data = pd.read_csv(uci.UCIHeartDiseaseDataFile.cleveland_preprocessed, names = uci.get_standard_features())\n",
    "print(f'Data shape: {data.shape}.')\n",
    "data.head(3)"
   ],
   "id": "b56fe2f0b56e8fd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Set Models' Hyperparameters and Cross-Validation Strategy",
   "id": "7190a84cd8056e45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verbose variable for log inspection.\n",
    "verbose = False\n",
    "\n",
    "# Dictionary to hold hyperparameters for grid and random search.\n",
    "parameters = dict()\n",
    "# The seed of the pseudo random number generated which is used while shuffling the data\n",
    "parameters['random_state'] = arange(0, 100, 1)\n",
    "# Inverse regularization parameter - A control variable that retains strength modification of Regularization by being inversely\n",
    "# positioned to the Lambda regulator. C = 1/Î»\n",
    "parameters['C'] = arange(0.0001, 10, 10)\n",
    "# Optimization\n",
    "parameters['solver'] = ['liblinear', 'newton-cg', 'lbfgs', 'saga']\n",
    "# Penalization (Regularization).\n",
    "parameters['penalty'] = ['l1', 'l2']\n",
    "parameters['multi_class'] = ['auto', 'ovr', 'multinomial']\n",
    "\n",
    "# A stratified K fold for cross-validation strategy - values are assigned from the evaluation helper module.\n",
    "skfolds = StratifiedKFold(n_splits = ev.kfold_n_split,\n",
    "                          shuffle = ev.kfold_shuffle,\n",
    "                          random_state = ev.random_state)\n",
    "\n",
    "# Dict for storing accuracies and drawing chart in the conclusion section.\n",
    "ev.accuracies = {}"
   ],
   "id": "dda875d83e7b6c7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Base Model (Function)",
   "id": "562faf4bcc050eb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# A function to build base model.\n",
    "def build_and_validate_base_model(x_train, x_test, y_train, y_test, test_name):\n",
    "    print(f'\\nEvaluation name: {test_name}.')\n",
    "\n",
    "    # Train the model with training set.\n",
    "    log_reg= LogisticRegression(verbose=verbose, random_state=ev.random_state)\n",
    "    log_reg.fit(x_train, y_train)\n",
    "\n",
    "    # Display the hyperparameters used.\n",
    "    print(f'Params         :{ log_reg.get_params()}.')\n",
    "\n",
    "    # Do prediction with the train model.\n",
    "    y_pred = log_reg.predict(x_test)\n",
    "\n",
    "    # Calls the evaluation helper module to display classification-report, confusion-matrix and ROC curve and return accuracy.\n",
    "    return ev.display_validation_report(y_test, y_pred, x_test, log_reg)"
   ],
   "id": "13d54e478f9733b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Grid Search CV (Function)",
   "id": "4f046cd4ab0bbe3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# A function discover the model's best hyperparameters and perform cross-validation using GridSearchCV.\n",
    "def build_and_validate_with_grid_search_cv(x_train, x_test, y_train, y_test, test_name):\n",
    "    print(f'\\nEvaluation name: {test_name}.')\n",
    "\n",
    "    log_reg= LogisticRegression(verbose=verbose, random_state=ev.random_state)\n",
    "\n",
    "    # Build Grid-search with parameters and stratified k fold strategy for cross-validation.\n",
    "    grid_scv = GridSearchCV(log_reg, parameters, scoring='accuracy', cv=skfolds)\n",
    "\n",
    "    # Train and discover the model's best hyperparameters for optimal performance.\n",
    "    grid_scv.fit(x_train,y_train)\n",
    "\n",
    "    # Display the best hyperparameters and score. The best score is mean of CV scores for train-set.\n",
    "    print(f'Best params         :{grid_scv.best_params_}.')\n",
    "    print(f'Best score (*mean)  :{grid_scv.best_score_}.')\n",
    "\n",
    "    # Predict using the trained model on the test-set.\n",
    "    y_pred = grid_scv.predict(x_test)\n",
    "\n",
    "    # Calls the evaluation helper module to display classification-report, confusion-matrix and ROC curve and return accuracy.\n",
    "    return ev.display_validation_report(y_test, y_pred, x_test, grid_scv)"
   ],
   "id": "708dd7e8caed6ca5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Random Search CV (Function)",
   "id": "b41571812e0534b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# A function discover the model's best hyperparameters and perform cross-validation using RandomSearchCV.\n",
    "def build_and_validate_with_random_search_cv(x_train, x_test, y_train, y_test, test_name):\n",
    "    print(f'\\nEvaluation name: {test_name}.')\n",
    "\n",
    "    log_reg= LogisticRegression(verbose=verbose, random_state=ev.random_state)\n",
    "\n",
    "    # Build Random-search with parameters and stratified k fold strategy for cross-validation.\n",
    "    rand_scv = RandomizedSearchCV(log_reg, parameters, scoring='accuracy', cv=skfolds)\n",
    "\n",
    "    # Train and discover the model's best hyperparameters for optimal performance.\n",
    "    rand_scv.fit(x_train,y_train)\n",
    "\n",
    "    # Display the best hyperparameters and score. The best score is mean of CV scores for train-set.\n",
    "    print(f'Best params         :{rand_scv.best_params_}.')\n",
    "    print(f'Best score (*mean)  :{rand_scv.best_score_}.')\n",
    "\n",
    "    # Predict using the trained model on the test-set.\n",
    "    y_pred = rand_scv.predict(x_test)\n",
    "\n",
    "    # Calls the evaluation helper module to display classification-report, confusion-matrix and ROC curv and return accuracy.\n",
    "    return ev.display_validation_report(y_test, y_pred, x_test, rand_scv)"
   ],
   "id": "f7f51728763c42b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation with Standard Dataset (without class balancing)",
   "id": "411bab791d873c1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preparation",
   "id": "5117a44a9e3e1e14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare features and target variables.\n",
    "X = data.drop(uci.UCIHeartDiseaseData.target, axis = 1)\n",
    "y = data[uci.UCIHeartDiseaseData.target]\n",
    "\n",
    "# Calls the evaluation helper module to scale features and do split (classes are balanced using stratify split).\n",
    "X_train, X_test, Y_train, Y_test = ev.scale_and_split(X, y)\n",
    "\n",
    "# Display the counter for '1' and '0' in y set before splitting.\n",
    "print('Class counter (stratified):')\n",
    "print(f'Entire-set  : {Counter(y)}.')\n",
    "print(f'Train-set   : {Counter(Y_train)}.')\n",
    "print(f'Test-set    : {Counter(Y_test)}.')\n",
    "\n",
    "std_y_data = [['Entire', Counter(y).get(0), Counter(y).get(1)],\n",
    "              ['Train', Counter(Y_train).get(0), Counter(Y_train).get(1)],\n",
    "              ['Test', Counter(Y_test).get(0), Counter(Y_test).get(1)]]\n",
    "\n",
    "pd.DataFrame(std_y_data, columns=['Set', '0', '1']).set_index('Set').plot.bar();"
   ],
   "id": "2e7bbc3fb749dc84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluation: Standard Base\n",
    "acc = build_and_validate_base_model(X_train, X_test, Y_train, Y_test, ev.EvaluationType.std_base)\n",
    "ev.accuracies[ev.EvaluationType.std_base] = acc"
   ],
   "id": "fa3dec4ac3dbc8c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluation: Standard Grid Search CV\n",
    "acc = build_and_validate_with_grid_search_cv(X_train, X_test, Y_train, Y_test, ev.EvaluationType.std_grid_search_cv)\n",
    "ev.accuracies[ev.EvaluationType.std_grid_search_cv] = acc"
   ],
   "id": "5ff133659b50a6f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluation: Standard Random Search CV\n",
    "acc = build_and_validate_with_random_search_cv(X_train, X_test, Y_train, Y_test, ev.EvaluationType.std_random_search_cv)\n",
    "ev.accuracies[ev.EvaluationType.std_random_search_cv] = acc"
   ],
   "id": "37bee9ea660bce7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f'Accuracies : {ev.accuracies}.')",
   "id": "46a53d7088d81e90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation with Oversampled Dataset (with class balancing)",
   "id": "36476ad66e0ed040"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preparation and Oversampling",
   "id": "bc4aaf4ceca7c794"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare features and target variables from original data frame.\n",
    "X = data.drop(uci.UCIHeartDiseaseData.target, axis=1)\n",
    "y = data[uci.UCIHeartDiseaseData.target]\n",
    "\n",
    "# Perform over-sampling with SMOTE\n",
    "X_b, y_b = SMOTE(random_state=ev.random_state).fit_resample(X, y)\n",
    "\n",
    "# Scale and spit the oversampled data.\n",
    "Xb_train, Xb_test, Yb_train, Yb_test = ev.scale_and_split(X_b, y_b)\n",
    "\n",
    "# Display the counter for '1' and '0' in y set before splitting.\n",
    "print('Class counter (stratified):')\n",
    "print(f'Entire-set  : {Counter(y_b)}.')\n",
    "print(f'Train-set   : {Counter(Yb_train)}.')\n",
    "print(f'Test-set    : {Counter(Yb_test)}.')\n",
    "\n",
    "os_y_data = [['Entire', Counter(y_b).get(0), Counter(y_b).get(1)],\n",
    "              ['Train', Counter(Yb_train).get(0), Counter(Yb_train).get(1)],\n",
    "              ['Test', Counter(Yb_test).get(0), Counter(Yb_test).get(1)]]\n",
    "\n",
    "pd.DataFrame(os_y_data, columns=['Set', '0', '1']).set_index('Set').plot.bar();"
   ],
   "id": "b6294d37456edbd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Before and after oversampling\n",
    "target_set_data = [['Standard', Counter(y).get(0), Counter(y).get(1)],\n",
    "                   ['Oversampled', Counter(y_b).get(0), Counter(y_b).get(1)]]\n",
    "\n",
    "pd.DataFrame(target_set_data, columns=['Set','0','1']).set_index('Set').plot.bar();"
   ],
   "id": "32ad6c2c1c2a9796",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluation: Over-sampled Base\n",
    "acc = build_and_validate_base_model(Xb_train, Xb_test, Yb_train, Yb_test, ev.EvaluationType.os_base)\n",
    "ev.accuracies[ev.EvaluationType.os_base] = acc"
   ],
   "id": "388a8e475befd119",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluation: Over-sampled Grid Search CV\n",
    "acc = build_and_validate_with_grid_search_cv(Xb_train, Xb_test, Yb_train, Yb_test, ev.EvaluationType.os_grid_search_cv)\n",
    "ev.accuracies[ev.EvaluationType.os_grid_search_cv] = acc"
   ],
   "id": "c7cca0fd48de349d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluation: Over-sampled Random Search CV\n",
    "acc = build_and_validate_with_random_search_cv(Xb_train, Xb_test, Yb_train, Yb_test, ev.EvaluationType.os_random_search_cv)\n",
    "ev.accuracies[ev.EvaluationType.os_random_search_cv] = acc"
   ],
   "id": "357c4277d918393d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f'Accuracies : {ev.accuracies}.')",
   "id": "3629168643044700",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conclusion",
   "id": "5fbb3decbf32b3da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to display value on the bar chart.\n",
    "def add_labels(x_pos, y_pos, delta):\n",
    "    for i in range(len(x_pos)):\n",
    "        dist = i + delta\n",
    "        plt.text(dist, y_pos[i], y_pos[i], ha = 'center')\n",
    "\n",
    "acc_model_type = ['Base', 'GridCSV', 'RandSCV']\n",
    "bar_width = 0.35\n",
    "x = np.arange(len(acc_model_type))\n",
    "\n",
    "# Bar Plot size\n",
    "plt.figure(figsize = (7,7))\n",
    "\n",
    "# Set standard bar type and values.\n",
    "acc_standard_data = [\n",
    "    ev.accuracies[ev.EvaluationType.std_base],\n",
    "    ev.accuracies[ev.EvaluationType.std_grid_search_cv],\n",
    "    ev.accuracies[ev.EvaluationType.std_random_search_cv]\n",
    "]\n",
    "# Set oversampled bar type and values.\n",
    "acc_oversampled_data = [\n",
    "    ev.accuracies[ev.EvaluationType.os_base],\n",
    "    ev.accuracies[ev.EvaluationType.os_grid_search_cv],\n",
    "    ev.accuracies[ev.EvaluationType.os_random_search_cv]\n",
    "]\n",
    "plt.bar(x - bar_width / 2, acc_standard_data, bar_width, label='Standard', color='skyblue')\n",
    "plt.bar(x + bar_width / 2, acc_oversampled_data, bar_width, label='Oversampled', color='lightgreen')\n",
    "\n",
    "# Display value on the bar chart.\n",
    "add_labels(acc_model_type,acc_standard_data,-(bar_width/2))\n",
    "add_labels(acc_model_type,acc_oversampled_data, bar_width/2)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xticks(x, acc_model_type)\n",
    "plt.legend(title='Regions', loc='lower right')\n",
    "plt.show();\n",
    "\n",
    "# For debugging values.\n",
    "print(f'Model types             : {acc_model_type}.')\n",
    "print(f'Standard accuracies     : {acc_standard_data}.')\n",
    "print(f'Oversampled accuracies  : {acc_oversampled_data}.')\n",
    "print(f'Accuracies (dict)       : {ev.accuracies}.')"
   ],
   "id": "cd696d82a7568819",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Since, GridSCV perform more extensive cross-validation compared to RandSCV to ensure model's stability, the highest score between\n",
    "# standard data and oversampled data for GridSCV is selected.\n",
    "grid_accuracies = [ev.accuracies[ev.EvaluationType.std_grid_search_cv], ev.accuracies[ev.EvaluationType.os_grid_search_cv]]\n",
    "\n",
    "print(f'Grid accuracies     : {grid_accuracies}.')\n",
    "\n",
    "# Set the global score for comparisons.\n",
    "ev.global_accuracies['LR'] = max(grid_accuracies)\n",
    "\n",
    "print(f'Highest accuracy    : {ev.global_accuracies}.')"
   ],
   "id": "5c5750b2ab2fd8d1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
